# -*- coding: utf-8 -*-
"""Full Code .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18lRusT7pRgG2B95eN5oRr05imiq547aQ
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# %matplotlib inline
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import StratifiedKFold
kFold = StratifiedKFold(n_splits=5)
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from  sklearn.metrics  import  accuracy_score , precision_score , recall_score,confusion_matrix,classification_report

"""# Data Exploration and Cleaning"""

df = pd.read_csv("Real Estate Data V21.csv")

import re

def clean_price(price_str):
    price_str = price_str.replace('‚Çπ', '').replace(',', '').strip()
    match = re.match(r'([\d\.]+)\s*([A-Za-z]+)', price_str)
    if match:
        value = float(match.group(1))
        unit = match.group(2).lower()
        if unit == 'cr':
            return value * 1e7
        elif unit == 'l':
            return value * 1e5
    else:
        try:
            return float(price_str)
        except:
            return None

df['Price_Cleaned'] = df['Price'].apply(clean_price)



from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Location_Encoded'] = le.fit_transform(df['Location'])
df['Balcony_Encoded'] = le.fit_transform(df['Balcony'])

"""# Model Building and Hyperparameter Tuning"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Drop rows with NaN in 'Price_Cleaned'
df.dropna(subset=['Price_Cleaned'], inplace=True)

X = df[["Total_Area","Baths","Price_per_SQFT",
        "Location_Encoded","Balcony_Encoded","Area_per_Bath"]]
y = df["Price_Cleaned"]

# 6) TRAIN / TEST SPLIT  +  SCALING
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.2,random_state=42)
scaler = StandardScaler()
X_train_s, X_test_s = scaler.fit_transform(X_train), scaler.transform(X_test)

# 7) BASELINE OLS
baseline = LinearRegression().fit(X_train_s, y_train)
def metrics(y,t): return {"MAE":mean_absolute_error(y,t),
                          "RMSE":np.sqrt(mean_squared_error(y,t)),
                          "R2":r2_score(y,t)}
results = {"LinearRegression (OLS)" : metrics(y_test, baseline.predict(X_test_s))}

# 8) GRID‚ÄëSEARCH TUNING
tune_cfg = [
    ("Ridge",     Ridge(),     {"alpha":[0.01,0.1,1,10,50,100]}),
    ("Lasso",     Lasso(max_iter=10000), {"alpha":[1e-4,1e-3,1e-2,0.1,1,10]}),
    ("ElasticNet",ElasticNet(max_iter=10000),
                                {"alpha":[1e-4,1e-3,1e-2,0.1,1],
                                 "l1_ratio":[.1,.3,.5,.7,.9]})
]
for name, est, grid in tune_cfg:
    gcv = GridSearchCV(est, grid, cv=5,
                       scoring="neg_mean_absolute_error")
    gcv.fit(X_train_s, y_train)
    best = gcv.best_estimator_
    results[f"{name} (tuned)"] = metrics(y_test,best.predict(X_test_s))
    print(f"{name} best params ‚Üí {gcv.best_params_}")

# 9) COMPARE
res_df = pd.DataFrame(results).T.round(2)
print("\nModel comparison:")
print(res_df)

# 10) BAR PLOT
res_df[["MAE","RMSE"]].plot(kind="bar", figsize=(9,5), title="Linear‚Äëfamily Models\nMAE & RMSE")
plt.ylabel("Error"); plt.tight_layout(); plt.show()

#PolynomialRegression

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

# Drop rows where 'Price_Cleaned' is NaN
df_cleaned = df.dropna(subset=['Price_Cleaned']).copy()

X_poly = df_cleaned[['Total_Area']]
y_poly = df_cleaned['Price_Cleaned']

# Split data into training and testing sets
X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y_poly, test_size=0.2, random_state=42)

# Create a pipeline for polynomial regression
# Degree of polynomial can be adjusted
poly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())

# Train the model
poly_model.fit(X_train_poly, y_train_poly)

# Make predictions
y_pred_poly = poly_model.predict(X_test_poly)

# Evaluate the model (using metrics like R-squared or Mean Squared Error)
from sklearn.metrics import r2_score, mean_squared_error
print(f"R-squared: {r2_score(y_test_poly, y_pred_poly)}")
print(f"Mean Squared Error: {mean_squared_error(y_test_poly, y_pred_poly)}")

# Optional: Visualize the polynomial fit
plt.figure(figsize=(10, 6))
plt.scatter(X_test_poly, y_test_poly, color='blue', label='Actual')
# To plot the polynomial curve, we need to sort the test data
sort_axis = np.argsort(X_test_poly.values.flatten())
plt.plot(X_test_poly.values[sort_axis], y_pred_poly[sort_axis], color='red', label='Polynomial Fit')
plt.title("Polynomial Regression Fit")
plt.xlabel("Total Area")
plt.ylabel("Price Cleaned")
plt.legend()
plt.show()



# prompt: create a decision tree based on this data train and predict the data

from sklearn.tree import DecisionTreeRegressor

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Regressor model
dt_model = DecisionTreeRegressor(random_state=42)

# Train the model
dt_model.fit(X_train, y_train)

# Predict on the test data
y_pred = dt_model.predict(X_test)

# Evaluate the model (optional, but good practice for regression)
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Decision Tree Regressor - RMSE: {rmse:.2f}")
print(f"Decision Tree Regressor - R-squared: {r2:.2f}")

# You can also visualize the decision tree if needed (requires graphviz)
# !pip install graphviz
# from sklearn.tree import export_graphviz
# import graphviz

# dot_data = export_graphviz(dt_model, out_file=None,
#                            feature_names=X.columns,
#                            filled=True, rounded=True,
#                            special_characters=True)
# graph = graphviz.Source(dot_data)
# graph.render("decision_tree") # This will save a PDF file

# To display the tree within the notebook:
# graphviz.Source(dot_data)



# prompt: create a logistic regression about these data

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

# Since logistic regression is for classification, we need a target variable that is categorical.
# Let's create a binary target based on whether the price is above the median.
df['Price_Above_Median'] = (df['Price_Cleaned'] > df['Price_Cleaned'].median()).astype(int)

# Define features (X) and the new binary target (y)
X = df[['Total_Area', 'Baths', 'Price_per_SQFT', 'Location_Encoded', 'Balcony_Encoded', 'Area_per_Bath']]
y = df['Price_Above_Median']

# Handle potential infinite values that might have resulted from division by zero in 'Area_per_Bath'
# Replace inf values with NaN and then drop rows with NaN in the features
X.replace([np.inf, -np.inf], np.nan, inplace=True)
X.dropna(inplace=True)
y = y[X.index] # Align the target variable with the cleaned features

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize and train the Logistic Regression model
log_reg = LogisticRegression(solver='liblinear', random_state=42) # Using liblinear solver for smaller datasets
log_reg.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = log_reg.predict(X_test_scaled)

# Evaluate the model
print("Logistic Regression Model Evaluation:")
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))



import matplotlib.pyplot as plt
# Visualize the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(df[['Total_Area', 'Baths', 'Price_per_SQFT', 'Location_Encoded', 'Balcony_Encoded', 'Area_per_Bath', 'Price_Cleaned', 'Price_Above_Median']].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Matrix of Selected Features and Target Variables")
plt.show()

import joblib

# Assuming 'best' model is the Ridge or best from GridSearch
joblib.dump(best, "price_model.pkl")
joblib.dump(scaler, "scaler.pkl")
joblib.dump(le, "location_encoder.pkl")     # for Location_Encoded
joblib.dump(le, "balcony_encoder.pkl")      # for Balcony_Encoded

le_loc = LabelEncoder()
le_bal = LabelEncoder()

# app.py
import streamlit as st
import numpy as np
import joblib

# Load model and encoders
model = joblib.load("price_model.pkl")
scaler = joblib.load("scaler.pkl")
le_loc = joblib.load("location_encoder.pkl")
le_bal = joblib.load("balcony_encoder.pkl")

st.title("üè† House Rent Price Predictor")

# User inputs
area = st.number_input("Total Area (sqft)", 100, 20000, 1000)
baths = st.selectbox("Number of Bathrooms", [1,2,3,4,5,6])
price_per_sqft = st.number_input("Price per Sqft", 100.0, 100000.0, 5000.0)

location = st.selectbox("Location", le_loc.classes_)
balcony = st.selectbox("Balcony", le_bal.classes_)

if st.button("Predict Rent Price"):
    loc_encoded = le_loc.transform([location])[0]
    bal_encoded = le_bal.transform([balcony])[0]
    area_per_bath = area / (baths + 1)

    X = np.array([[area, baths, price_per_sqft, loc_encoded, bal_encoded, area_per_bath]])
    X_scaled = scaler.transform(X)

    predicted_price = model.predict(X_scaled)[0]
    st.success(f"üè∑Ô∏è Predicted Rent Price: ‚Çπ {predicted_price:,.0f}")
